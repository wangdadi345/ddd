{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8632fd51-7f8d-4bb5-af10-aafaa627a959",
   "metadata": {},
   "source": [
    "# 代价函数\n",
    "\n",
    "- 平方茶误差函数，找到最小平方差，用来解决回归问题‘\n",
    "\n",
    "- 两个函数，一个变量是x结果是y，另一个是求方差的函数变量就是x的参数\n",
    "\n",
    "# 如何做作业\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2fe54-68dd-457e-a18e-5a6f00010942",
   "metadata": {},
   "source": [
    "#  其中的线代知识\n",
    "\n",
    "- 其中矩阵（相当于二维数组从1开始）用A表示，向量（单列矩阵）用y表示。矩阵乘以向量等于向量。A_mn * y_n1 = y_m\n",
    "\n",
    "- *矩阵乘以向量*就是 将矩阵的一行与向量的转置相乘再相加，作为新向量的一元\n",
    "\n",
    "- 矩阵相乘就是将第二个矩阵分成多个向量\n",
    "\n",
    "- *矩阵小技巧1*在程序中将多个多个未知量代入一个已知方程中时，可以构建两个矩阵，前者存放要代入的未知量一行表示方城中所有的未知量，n行表示n次代入\n",
    "\n",
    "- *矩阵小技巧2*多个数据代入不同方程就是两个矩阵相乘\n",
    "\n",
    "- 矩阵乘以同纬度的单位矩阵结果不变，可以交换，矩阵的计算符合结合律不符合交换律\n",
    "\n",
    "- 矩阵的逆运算(全是0无逆运算) A * A‘ = A’ * A = 1，并且该矩阵一定要是行列相同的（名叫方阵），一般电脑自带逆矩阵的计算 octave中函数A = [10 2 ,13 6]pinv(A)\n",
    "\n",
    "- 函数eye(n)会返回一个n * n的单位矩阵\n",
    "# 多元线性回归方程（多元x）\n",
    "\n",
    "-根本目的是找到一个曲线是他与样本数据的铁和度更高 ，\n",
    "\n",
    "- n表示元的个数而m表示样本数量。因为是多元，所以可以用x作为一个向量表示全部的元。x(i)_j表示第i个样本中的第j个特征变量、\n",
    "\n",
    "- 线性回归的假设方程 h = i_0*x_0 +i_1*x_1+...+i_n*x_n,变量x为一个n+1维向量，而参数i也是一个n+1维向量。x=[x_0,x_1,....x_n]并认为x_0 = 1，将参数i向量进行转制，\n",
    "即可进行计算，方程就可简写成 h = x * i_T,在octave中矩阵的逆运算用函数pinv\n",
    "\n",
    "- 多元梯度下降法 j(i) = ***\n",
    "\n",
    "## 梯度下降计算的实用技巧（特征缩放）\n",
    "\n",
    "- *特征缩放* 当x——1与x——2的取值相差过大时，i_1与i_2的图像就是一个多个椭圆重叠的形状，差距越大，椭圆越瘦就越难找到最小值。就需要用到特征缩放也可以理解为等比例缩放\n",
    "比如将他们同时除以最大值，使得范围缩小。目的是适量缩小变量x的取值范围的差距（一般在-3到3与-1/3到1/3就行）\n",
    "\n",
    "- *归一化* 对变量进行减法与除法，使变量最大值与最小值和为0\n",
    "\n",
    "- 学习率  可以理解为在参数变化时参数偏导的参数，决定了变化幅度，在一个以迭代次数为横轴，函数j的最小值为纵轴的图中（正常情况下他应该是递减的，并且收敛的），当图形开始递增时一般是因为学习率太大。\n",
    "学习率一般是从1开始根据情况乘或除以3（四舍五入）.\n",
    "\n",
    "- *选取合适的算法变量的*比如一块地的价钱，相对于长宽两个变量，可以之用一个面积变量。\n",
    "\n",
    "- *多项式回归（也是对变量的操作）*有时候训练集由两个方程二次方程与三次方程组成的多项式方程（或1/2次\n",
    "表示更加贴切。具体做法就是将方程h = i_0*x_0 +i_1*x_1+...+i_n*x_n,中的x_n换成x的n次方（n就是多项式的最高次）\n",
    "\n",
    "## 直接求解不同于迭代法（公式法）\n",
    "\n",
    "- 根据方程 pinv(A' * A)*A'*Y ,将m个样本的每一个(每个样本可能有n个特征变量)x作为一个向量(n+1维)，再将这个向量转置后填入矩阵中形成一个m\\*(n+1)维的矩阵A\n",
    "\n",
    "- 当矩阵是不可逆的时候用函数pinv,可逆的时候就用inv\n",
    "\n",
    "## 两种方法的优劣\n",
    "\n",
    "- *递归法*操作计算简单但麻烦要多次迭代而且要确定学习率，后者则是快速但是当特征变量太大是计算量过大，一般特征变量大于1000时选择前者\n",
    "\n",
    "# Octave（与C语言不同1就表示第一个）\n",
    "\n",
    "- **语法** 注释用% 不等于是： ～= 等于： == 与： && 或：|| 异或运算(逻辑值相同返回0反之1还有位运算的作用暂时没学): xor(,) 改变运算提示符：PS1('改变后的提示符');\n",
    "符号 ；当代码未完成你还不希望输出结果时 赋值操作一样， 要输出变量的大小时直接输入变量即可不需要print等或者用函数disp  输出字符串： disp(sprintf('***** %0.2f',a))\n",
    "显示变量的默认位数 ： format long 位数尽量小： format short\n",
    "\n",
    "- **线代** 建立矩阵： A= \\[ 1 2;3 4; 5 6] ；表示换行 % \\用来消除md中【】的作用 向量 行向量 V = \\[1 2 3] 列向量 V=\\[1 ;2 ;3 ;]  特殊向量 v= 1:0.1:2 从1开始每次加0.1直到2的行向量 v = 1:6 从1到6的整数向量  生成矩阵 ones(2,3) 变量都是 1 2*ones(2,3) 变量都是2 函数zeros用法一样变量为0 rand(1,3)随即生成一个 变量在0到1之间的 1\\*3的矩阵  randn(2,3) 矩阵的变量满足 均值为0方差为1 hist(向量或矩阵，条形个数)绘制一个条形图 eye(n)生成一个n * n 的单位矩阵\n",
    "\n",
    "- help 函数名 调出函数的相关信息，size(A，1)查询矩阵A的大小，1表示查询第一维度的大小，返回一个矩阵储存,who可以显示文件中的所有变量whos显示更详细的变量\n",
    "\n",
    "- A=[A,向量]新增一行或一列，C=[A B];C=[A;B]\n",
    "\n",
    "## 调用数据\n",
    "\n",
    " - pwd显示octave的文件路径，函数 load + 文件名 之后该文件会以变量的身份存在，clear +变量 删除变量 ，单独一个clear 删除全部变量 save 文件 + 变量 表示将变量以二进制形式储存在该文件中（文件可以不存在）后加 - ascii 表示以文本形式储存\n",
    " \n",
    "## 数据的运算\n",
    "\n",
    "- A * B矩阵的相乘 A .* B 当两矩阵形式相同时 表示各个元素相乘 A .^2 各个元素平方 . 就表示对矩阵所有元素进行计算  log(A)取对 exp(A)表示以e为底的幂运算 abs取绝对值 A（向量） + ones(length(A),1) 将A 中每个数加1或A + 1 A'表示转制（python中表示为 A.T）\n",
    "\n",
    "- max(A)显示每一列的最大值，max(A(:))找最大值 [max(A),ind] = max(A)多返回其对应的引索，a < 1 返回0与1的矩阵find(a<1)返回引索  [m,n]=find(A<3) 返回两个矩阵分别表示行和列 magic(n)f返回一个n维特殊矩阵幻方 sum(a)prod(a)求所有元素的和，乘积 floor(a)向下取整ceil(a)向上取整\n",
    "\n",
    "- max(rand(3),rand(3))随即生成两个3*3矩阵中最大值 max(A,[],1/2)1第一维度（行维）是所有列的最大值2是行sum(sum(A.*eye(n)))求A的对角线的总和\n",
    "\n",
    "## 绘图并将数据可视化（与matcloblid很像）\n",
    "\n",
    "- 函数hold on在原图的基础上绘制新图\n",
    "\n",
    "- print -dpng 'My_plot.png' 保存图件进文件中，输入heip plot可以查看其他的图片类型，figure(1)为图片命名\n",
    "\n",
    "- subplot(1,2,1)将图片分为两个格子使用第一个格子，在plot后加上axis([0.5 1 -1 1])前两个是x轴范围后两个是Y轴， clf清理\n",
    "\n",
    "-  A = magic(5)  imagesc(A) colorbar colormap  gray将A可视化后两个用到在查\n",
    "\n",
    "-  使用逗号连续执行函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f6816-c9cf-4cce-8d65-5dac21d2a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.0:0.01:0.98];\n",
    ">> y1=sin(2*pi*4*t);\n",
    "y2=cos(2*pi*4*t)\n",
    ">> subplot(1,2,1)\n",
    "figure(1);plot(t,y1)\n",
    ">>hold on\n",
    "subplot(1,2,2)\n",
    "figure(2);plot(t,y1,'r')\n",
    "xlable('time')\n",
    "ylable('value')\n",
    "legend('sin','cos')\n",
    "title('my plot')\n",
    "print -dpng 'My_plot.png' \n",
    "subplot(1,2,1)\n",
    "close 关闭图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ead067-1f25-47d7-8a96-1032f82b96b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '（' (U+FF08) (1870957466.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    - 基本与python相同 不用缩进但在循环与判断语句最后要用到end（必要的）,而且最后:变成,（可有可无），elseif连着的 最后输入quit推出octave\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '（' (U+FF08)\n"
     ]
    }
   ],
   "source": [
    "## 控制语句\n",
    "\n",
    "- 基本与python相同 不用缩进但在循环与判断语句最后要用到end（必要的）,而且最后:变成,（可有可无），elseif连着的 最后输入quit推出octave\n",
    "\n",
    "- 文件后最.\n",
    "\n",
    "## 定义与使用函数\n",
    "\n",
    "- 首先需要创建一个以函数命名的文件,并且在调用前将octave调至文件对应的目录中（cd）高级技巧 search path 相当于给octave添加一个默认的路径即使其不再该目录中也会主动查找\n",
    "例子: addpath('C:\\User\\ang\\Deaktop')\n",
    "\n",
    "- 可以返回多个变量,但此时需要一个列表来接收"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746ee50-33e9-4151-971d-65057178cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "function [y1,y2,y_n] = 函数名(x)   % y 是返回变量，x是输入的自变量\n",
    "y = x^2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d68cc-5a25-452c-b55d-da9948b21a60",
   "metadata": {},
   "source": [
    "### 向量\n",
    "\n",
    "- 数学上的累计求和就可以看作是向量的乘积，原本所学的递归方程中参数的更新方程就可以看作是向量的乘积\n",
    "\n",
    "### （逻辑问题）logistic(sigmoid)算法（用于解决分类问题，其输出值在0到1之间）\n",
    "\n",
    "- t 表示参数的向量 例如 h(x) = t' * x ,其输出结果可能会超出0到1,于是用logistic函数 令 z = t' * x ,g(z) = 1/1+exp(-z),根据图像可知当z >= 0时，y>=0.5,即y取值为1.\n",
    "\n",
    "- 最终结果是计算对应变量输出为1的概率\n",
    "\n",
    "- *决策边界*，由参数决定，即在图像上用于判断y是0还是1的一般该曲线是y = 0.5的那条线，当这条线明显不是直线时可以增加变量的多项式\n",
    "\n",
    "- *更新代价函数*由于logistic函数及其非线性，所以会影响最优参数的寻找，所以我们要寻找另一个代价函数（求y的函数） 记 ：h(x)是预测值（概率） y是实际值（0或1）\n",
    "代价值cost  = (y = 0) -log(1-h(x)) 或 （y = 1）=-log(h(x))  J(x) = 1/m * cost(求和)，J(x)越小说明曲线越贴和\n",
    "\n",
    "- *更简单的代价函数* 不再区分Y的大小 cost(h(x),y) = -y * log(h(x)) - (1-y) * log(1-h(x))\n",
    "\n",
    "- *拟合最优参数* 梯度下降法 参数的更新与以前一样\n",
    "\n",
    "#### 对于logistic函数相对于梯度下降的最优解\n",
    "\n",
    "- **共轭梯度法** Bfgs  L—BFGS这三种方法不需要考虑学习率，速度快  缺点是很复杂\n",
    "这些算法储存在一个库中\n",
    "\n",
    "- ![截图 2025-03-04 10-27-30](/home/wangyuhao/图片/截图/截图 2025-03-04 10-27-30.png)\n",
    "\n",
    "- 函数fminunc(无约束最小化函数)要使用它需要设置 options（数据结构）需要用到gradient表示参数变化对应的梯度 指针表示为 @ + 指向的对象\n",
    "\n",
    "- initialTheta是储存参数的向量 Octave计数从1开始 gradient表示参数变化对应的梯度 jVal是代价函数的结果\n",
    "\n",
    "#### 逻辑分类 ，对于多元问题的 一对多的分类算法（曾经是二元现在是多元）\n",
    "\n",
    "- 例子： 比如你需要将文件放在不同的文件夹中（多元）用Y = 1 2 3 n 来表示\n",
    "\n",
    "- 做法，将他们分成多个二元问题 ，如 Y=1和Y～=1 从而得到n个分类器（得到概率）\n",
    "\n",
    "- 接着将变量分别输入这N个分类其中寻找最大的概率\n",
    "\n",
    "## 过度拟合问题||正则化\n",
    "\n",
    "- 比如一个模型需要3个参数 2个叫欠拟合 4个叫过拟合（符合老样本但不一定适用新样本）\n",
    "\n",
    "### **当调试使模型出现问题时的做法（识别过拟合与欠拟合）**\n",
    "- *通过绘制图像*\n",
    "- *尽量减少变量*（删去不太重要的），缺点是模型的可信度相对下降\n",
    "- *正则化*减少量级或参数大小\n",
    "#### 正则化(线性回归)\n",
    "- *部分惩罚*在原来的代价函数的基础上再加上 1000* 参数（[3,4]），这样可以在代价函数最小时，参数[3,4]也尽可能小（被叫做惩罚）\n",
    "- *全部惩罚* 但有时你不知道要惩罚谁，于是也可以加上  t * sum(theta[n].^2)(一般不考虑theta_0) ,其中参数t用来调整公式之中两部分之间的关联 t太大就会导致theta太小\n",
    "- *全部惩罚对应的参数更新（迭代法）* theta_j = theta_j - a[1/m * sum(h(x_i)-y_i) * x(i) + t/m *theta_j]\n",
    "即 原方程 theta_j但参数从1变成了（1 - a*t/m）一般a较小m较大\n",
    "- *对于公式法的影响*theta = (X` 叉乘 t[矩阵A]^-1) * X`*y  A是一个n+1维的矩阵左对角线第一个为0其余为1\n",
    "##### 正则化（逻辑回归）\n",
    "- *logistic 算法* ，与全部惩罚的操作相同，就是加一项尽管章的很像但是由于假设不同这是两个算法\n",
    "###### 更高级的优化算法 (有问题怎么把costFunction最小化)\n",
    "- 对此我们要做的就是自己定义一个函数costFunction输入theta的向量输出代价函数的结果与参数变化对应的梯度（即参数求导的那个变化率）\n",
    "- 将定义的函数放入函数fminunc(@ costFunction,theta_0)将cost函数最小化\n",
    "## 非线性假设（神经网络，特征变量太多时）\n",
    "- 特征变量太多时线性回归与logistic函数不适用了\n",
    "### 神经网络（模仿神经大脑）\n",
    "- 激活函数指的是带有logistic函数 用来计算非线性问题的，在该领域theta一般被称为模型的权重或参数 有时u会有偏执单元都为1\n",
    "- **神经网络**是有多组激活函数组成的带有输入输出的，其中第一层被称为输入层（输入特征变量）；中间几层被称为隐藏层（训练集中看不到的）\n",
    "-  **特定术语**a^(j)_i表示第j层第i个神经元的激活项，激活项表示一个神经元计算并输出的结果（不一定在第三层也能在一二层）\n",
    "-**重点** 在神经网络中，\"连接第 j 层和第 j+1 层的权重矩阵\"是指用于将第 j 层的输出映射到第 j+1 层输入的矩阵，\n",
    "    如果神经网络在第 j 层有 s_j 个单元，在第 j+1 层有 s_{j+1} 个单元，那么连接第 j 层到第 j+1 层的权重矩阵的维度是（s_(j+1)）* (s_j+1)(后边加一是因为有个偏执单元)、\n",
    "- theta作为激活项的权重矩阵时形式为theta^(j)_ni,j表示第j层，n表示第j+1层的第n个激活项的输入，i表示第j层的第i（有0）个输入结果\n",
    "#### 进行矩阵的计算(前向传播)\n",
    "- 输入层可以理解为一个X_1矩阵存放输入值，然后第二个矩阵Theta_2储存每个激活相对应的权重向量 g(X1 * X2)就得到了隐藏层的输出结果的向量\n",
    "- a_1表示第一层的输出向量a_2表示第二层的输出向量z_2=a_1*thete_1   \n",
    "- 用神经学习计算 and (not)and(not) or 通过这三个简单的神经模拟可以模拟更复杂的 XNOR 将前两个算法作为隐藏层 or作为输出层就可以模拟逻辑运算符XNOR、\n",
    "#### 神经学习下的代价函数\n",
    "- L表示神经学习的总层数 s_l表示第l层的单元数（）不包括编制单元\n",
    "- 二元分类问题输出结果只有1或0,多类别问题输出结果为K维向量，有K的输出结果\n",
    "#### 反向传播（计算代价函数的最小值，计算偏导向）\n",
    "-新变量 p^(l)_j 第l层第j个激活值的误差，（p^(4)_j）输出层的就是计算值减去实际值 \n",
    "- p^(n)_j=theta^(n)' * p^(n+1).*g'(z(3)) .*表示数学的的乘法 g'表示求导，化简得到p^(n)_j=theta^(n)' * p^(n+1).*a^(3).\\*(1-a^(3))\n",
    "- 输入层无偏差\n",
    "- set sin^(l)_ij = 0(for all ij)\n",
    "#### 神经学习的细节操作 \n",
    "- *矩阵*matrices矩阵函数matrices(Thata1,Theta2,Theta3)\n",
    "- 展开参数**将参数从矩阵转变成向量**因为实际操作时函数要求参数为向量，在神经网络中参数为矩阵，（只是变量名）ThetaVec=[Theta1(:);Theta2(:);Theta3(:)],将参数矩阵的所有项展开排列形成一个长向量。\n",
    "- 将向量传入函数后又需要将这几个矩阵提取出来操作：reshape(ThetaVec(1:111))\n",
    "- **梯度检测（在反向传播时很难发现bug,有这个方法来检验bug）**\n",
    "- \n",
    "- **theta为实数时**用定义求导数  gradApporox = (J(theta+EPSILON)-J(theta-EPSILON))/( 2*EPSILON)\n",
    "- **theta为向量时（多变量方程）** 求偏导，但还是用上述方法\n",
    "- **在octave上的操作**for i = 1:n，  \n",
    "thetaPlus = theta;  \n",
    "thetaPlus(i)= thetaPlus(i) + EPSILON; thetaMinus = theta;  \n",
    "thetaMinus(i)= thetaMinus(i) - EPSILON;  \n",
    "gradApprox(i)= (J(thetaPlus) - J(thetaMinus))  \n",
    "/(2*EPSILON);  \n",
    "end;\n",
    "- 将计算结果与函数返回的导数进行比较若近似相等可以\n",
    "- 当你在进行代码的学习训练时记得将梯度检测关了，因为梯度计算的计算量很大\n",
    "- **随即初始化**\n",
    "- \n",
    "- 对于高级运用算法，他通常会默认你提供了参数的初始值，当初始化都为0时，被称为对称权重问题，无意义所以需要正确的初始化\n",
    "- - Thetal = rand(10,11)*(2*INIT_EPSILON)- INIT EPSILON;\n",
    "- Theta2 = rand(1,11)*(2*INIT_EPSILON)- INIT EPSILON\n",
    "- rand函数生成一个矩阵单元在0到1之间\n",
    "##### 计算偏导的步骤\n",
    "- 先将训练集里的值代入，接着前向传播，在反向传播算偏差值 最后 sin^(l)_ij = sin^(l)_ij + a^(l)_j * p^(l+1)_i\n",
    "**向量化** sin^(l) = sin^(l) * p^(l+1)*a^(l)'\n",
    "- D^(l)_ij := 1/m * sin^(l)_ij + t*theta^(l)_ij (j~=0)\n",
    "- D^(l)_ij := 1/m * sin^(l)_ij  (j==0)\n",
    "- D就是对应参数的偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae804f9-6608-4586-9439-6ba797631f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
